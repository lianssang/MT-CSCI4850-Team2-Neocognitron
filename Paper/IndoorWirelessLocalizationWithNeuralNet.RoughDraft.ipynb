{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indoor Wireless Localization with Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Manoj Basnet, Steven Hearne, Lian Sang, Gregory Wagner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ABSTRACT:\n",
    "    \n",
    "The next generation of GPS like navigation lies within the doorstep of modern buildings. With the use of mobile devices and the trending of user-friendly applications, indoor wireless localization is a possibility in today's society. By taking advantages over Recieved Signal Strength (RSS) of WiFi signals and fingerprint database, the existance of GPS-like indoor is near in the future. However, the trouble of indoor localization is that the RSS values on the reference device and target device are not quite the same, or exact, therefore it has cause errors in localization. To capture and minimize the errors, a method of multilayer network will be used. In order to proceed this method, fingerprinting a number of locations that one desire within the indoor space is required. As mentioned that the a reference device is needed, therefore a WiFi equipped laptop is set up for each location to be fingerprinted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index Terms - Indoor localization; wireless networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. INTRODUCTION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GPS has been a very functional tool that provides many beneficial factors for society from advancing technology of auto-pilot vehicles to calculation on movements of the Earth's surface, as to indoor localization the potential may also be numerous, especially for civilians. Knowing that many buildings in today's populated cities, such as residential houses and commercial zones are equipped with wireless technology, by taking a step further to advancement of navigation could possibly be another breakthrough in the waiting. Imagine a single tap away from navigating indoor without the help of another person except for a turn-by-turn visualization mapping that feeds out, the benefit can be viewed quite substantial to several fields, whether it is for public safety or private investment. With the capability that indoor wireless localization holds, the technology extends the nature of safety by monitoring senior citizens and children from endangerment. As for luxurious use of the localization technology, many commercially-based companies may lure in consumers based on their current location by the user's shopping interests. It is a win-win situation for consumers and merchandisers with the help of indoor localization technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. BACKGROUND:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As unfortunate as today's GPS systems are not meant to be used for indoor environment, it is time that the tech community introduce a new method of wireless localization for inside buildings. Starting with the selection of signal, which is a crucial drawbacks of GPS for indoor, there are several signal types that can accommodate indoor localization: common ones like FM radio that got used in [1] and the used of visible light in [2], and a potential signal that could extend GPS signal with pseudolite in [3]. Given that these various types of signal could possibly extend the navigation dream, WiFi signal that is used in [4] could potentially be the best candidate for this project. Since WiFi signals are being used nearly everywhere in our modernize environment, it is ideally a smart choice also considering the fact that a user's device, such as a smartphone or tablet, is already equipped with built-in WiFi capable hardware.\n",
    "After deciding the systematic signals for wirelesss localization, techniques and methods are w When it comes to the localization of indoor wireless, there is also a selection for the techniques in which that can be done. Therefore, just like selecting a signal type, it can be such a bold move to choose any type of wireless method of navigation to alternate an indoor version of GPS. This is because there are several techniques for wireless indoor localization, which can be found in [5], for each with different drawbacks. As for the convenience of measurement issue, mapping methods are required due to the conversion of physical measurements to environmental locations. selecting an appropriate method of techniques is severely important. Depending on the different types of mapping the techniques are varies. According to [4], the most representable type of mapping would be geomatric mapping due to the complexity of the environment, therefore the specific mapping will require fingerprinting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III METHODS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA:\n",
    "\n",
    "An appropriate step to approach this project would be to obtain some form of data sets. Since the study is dealing with WiFi localization, therefore some very important key words (numerical data) such as: number of scans, training vectors, locations, and wireless access points (APs), etc. are nescessary to be trained using neural network. Depending on the Neural Network Model one choose to exercise, the values previously listed are to be adjusted accordingly. The training data for the Neural Net will need to be set up in a dataset that consists of the label with feature vectors. This dataset for example should look something like [loc 0 - number of choice], and its features should be RSSI values from the APs. The difficult part to this dataset is that one should plan the number of locations according to the environment and figuring out the count of APs that is necessary to satisfy the project. Here is an example vector:\n",
    "\n",
    " AP1  | AP2 | AP3  | AP4 | AP5 |  location  \n",
    " ----:|----:| ----:|----:|----:|----------:|\n",
    " -50  | -74 | -66  | -55 | -60 |0          |\n",
    "\n",
    "As for raw data, using WifiInfoView.exe program should do the trick. If one is on a Linux machine, then it is necessary to 'wine' to run a Windows executable. Keep in mind that if it envolves the use of Jupyter notebook and while being on a remote system, then WiFi information being collected is not from ones machine. For the db_wifiinfo.bat and db_wifi_bash.ipynb files, these are for Windows/Linux respectively and will run the .exe program a number of times to produce scans of any location one choose to be. For this, also keep in mind that a robust data set is data that is taken over different periods of time, this is because of the nature of signal interference in APs. However, the tendency for the scans to produce NULL data where a signal has been momentarily lost, it is a good idea to get at least 500 scans per desired location for the Net. In provided examples, the data is taken continously (1 - max_loc_scans). In this experiment the data was initially taken and spread into vector files in order to find an average of 10 scans, to deal with some of the signal loss that will inevitably occur.  We found that this averaging had a severe effect on the variance the data should have represented to train the network properly.  In this case, one should take 500 or more scans at each location moving from location to different location and remember to space them out over time and allow small variance in time for the raw data. By following this method, one should now be able to take all the raw data that is neccessary for this study. \n",
    "\n",
    "As for processing the data that is obtained, ensure that the raw data is set up in location folders, in this case (loc_1_MAXLOC), in this example vector sub folders (vec_1-MAXVEC) exist but are not required, and a number of scans likely five to ten scans (dbXX.csv which XX is 1-MAXSCANS) inside the vector folders. The scans can be allocated (1-MAXSCANS) in each loaction folder.  If data is sorted as in this experiment, reusing the data_to_vector.ipynb to sort the vector files should be an appropriate choice. One should note that clean_loc_data is a bash file for removing vec_X folders from each loc_X folder quickly.  One could alter these simple scripting tools to thier desired setup.\n",
    "\n",
    "Once the files are correctly setup, one should modify the number of key words inside of builddata.ipynb then should run the script to produce a training set for the Neural Net.\n",
    "\n",
    "A set of testing data should be taken from each location in about 20 scans, or however many it takes to have at least a few vectors that do not have nan values.  These can be sorted in much the same way as the training locations.\n",
    "\n",
    "It should be noted that 3000 scans were taken for the three locations total, and only around 1500 provided valid training vectors for the network.  So one can conclude the raw data mechanisms are about 50% efficeint.\n",
    "\n",
    "#### PCA:\n",
    "\n",
    "In our PCA we are able to get a better understanding of how the Neural Network will view our data and make decision boundries.  The data is read in along with the planned testing locations, we then use Singular Value Decomposition to find the variance attributed to each feature.  Then we use the first two features, where approximately 54% of our variance lies, and plot our data along these as basis vectors.  In our PCA we found fairly clear descision boundries that could be handled by a Multi-layered Neural Network with non-linear activation functions.  Our testing data was also included, to see if our network was able to group the testing and training data similarly.  We found that our network would be able to do so.\n",
    "\n",
    "#### Network:\n",
    "\n",
    "The Neural network needed for this task is fairly small and simple. Our Neural Network was composed of two dense layers, two with non-linear activation function relu and sigmoid and 64 hidden units each.  The last was a layer with a hidden layer of three and a softmax activation function because we had three location categories.  This network was fairly small and extremely quick to train. We initially we tryed to make a even smaller network and train it with more epochs but the data set proved hard to train with that small of a network and the predictions where never as accurate. This grouping of activation functions relu and sigmoid seemed to be the most reliable for our data set. Tryed a single layer network and had no luck getting it to learn the data. This network learned the data set so quickly we did not see the need to make the network bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IV. RESULTS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V. DISCUSSION:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REFERENCES:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
